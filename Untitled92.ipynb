{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN Architecture | Assignment**"
      ],
      "metadata": {
        "id": "DMvFmroVEACz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: What is the role of filters and feature maps in Convolutional Neural Network (CNN)?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Filters in a Convolutional Neural Network (CNN) are small, trainable matrices that slide over input data (like images) to perform convolution operations and extract features such as edges, shapes, or textures. Each filter specializes in detecting certain patterns by calculating dot products at every position. As training progresses, the weights in these filters are optimized to focus on features relevant to the task.\n",
        "\n",
        "When an input passes through a filter, the output is called a feature map. Feature maps highlight where in the input specific patterns or features are present. Each convolutional layer can have many filters, so it generates multiple feature maps, each corresponding to a different learned feature.\n",
        "\n",
        "In the early layers of a CNN, filters learn to detect simple visual cues like edges or corners. In deeper layers, filters combine these simple patterns to identify complex shapes, textures, or even entire objects. This hierarchical approach allows CNNs to learn rich and complex feature representations directly from raw data.\n",
        "\n",
        "Feature maps are essential because they provide the intermediate representation of input data for subsequent layers to process, leading to robust recognition abilities. Their spatial organization retains locality information, which helps the network understand exactly where features appear in the image.\n",
        "\n",
        "More filters result in more feature maps and a more expressive model, but this also increases computational requirements and can risk overfitting if not managed carefully. Typically, after each convolution, activation functions (like ReLU) are applied to feature maps, introducing non-linearity, and pooling layers reduce spatial dimensions, keeping only the most salient information.\n",
        "\n",
        "Overall, filters are the primary tool for automatic feature extraction in CNNs, while feature maps contain the learned characteristics that enable accurate predictions or classifications in deep learning tasks."
      ],
      "metadata": {
        "id": "zKz4WXgJEDKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?**\n",
        "\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Padding and stride are two fundamental concepts in convolutional neural networks that influence how convolution operations are performed and directly affect the size of the output feature maps.\n",
        "\n",
        "**Padding** refers to the process of adding extra pixels around the border of the input image before applying the convolution operation. The main purpose of padding is to control the spatial dimensions of the output feature maps. There are mainly two types of padding:\n",
        "\n",
        "- **Valid Padding (no padding):** No extra pixels are added, so the filter only convolves over valid parts of the input. This causes the output feature map to shrink compared to the input size.\n",
        "- **Same Padding:** Pads the input with zeros so that the output feature map has the same spatial dimensions as the input after the convolution.\n",
        "\n",
        "Padding helps preserve edge information in images and allows deeper networks to maintain spatial dimensions over multiple layers.\n",
        "\n",
        "**Stride** is the number of pixels that the convolutional filter moves or \"strides\" over the input image during the convolution operation. It controls how densely the filter scans the image:\n",
        "\n",
        "- **Stride = 1:** The filter moves one pixel at a time, producing a high-resolution output feature map.\n",
        "- **Stride > 1:** The filter jumps multiple pixels, reducing the size of the output because it covers less spatial area during convolution.\n",
        "\n",
        "Stride effectively downsamples the input by controlling how much overlap occurs between receptive fields of neurons in the feature map.\n",
        "\n",
        "The formula to calculate the output dimensions of a feature map for one spatial dimension (height or width) given input size W, filter size F, padding P, and stride S is:\n",
        "\n",
        "$$\n",
        "\\text{Output dimension} = \\left\\lfloor \\frac{W - F + 2P}{S} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- $$W$$ = input dimension  \n",
        "- $$F$$ = filter size  \n",
        "- $$P$$ = padding size  \n",
        "- $$S$$ = stride length  \n",
        "\n",
        "This calculation shows that increasing padding can increase the output size, while increasing stride decreases the output size.\n",
        "\n",
        "In summary, padding helps control how the borders of the input are treated, often preserving the input size, and stride controls the step size of the filter, affecting the resolution and size of the output feature map. Proper tuning of padding and stride is crucial for designing CNN architectures that balance feature extraction ability with computational efficiency.\n"
      ],
      "metadata": {
        "id": "XSQZPBWVElTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3: Define receptive field in the context of CNNs. Why is it important for deep architectures?**\n",
        "\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "In the context of Convolutional Neural Networks (CNNs), the **receptive field** refers to the specific region of the input image that influences the activation of a particular neuron in a given layer. In simpler terms, it is the spatial area of the input that each neuron \"sees\" or is responsive to when computing its output.\n",
        "\n",
        "At the first convolutional layer, the receptive field of a neuron is essentially the size of the filter or kernel applied to the input image. However, as we move deeper into the network, the receptive field of neurons in deeper layers increases because these neurons aggregate information from multiple neurons in the previous layer, which themselves are linked back to a larger area of the original input.\n",
        "\n",
        "The concept of the receptive field is important for several reasons in deep CNN architectures:\n",
        "\n",
        "1. **Capturing Contextual Information:** A larger receptive field allows neurons in deeper layers to capture more global and contextual information about the input image, which is essential for understanding complex patterns and structures beyond simple edges or textures.\n",
        "\n",
        "2. **Hierarchical Feature Learning:** CNNs learn hierarchical features, starting with low-level details (like edges) in early layers and progressing to high-level abstractions (like object parts or entire objects) in deeper layers. The growing receptive field facilitates this hierarchical learning by integrating local information from earlier layers into more comprehensive features.\n",
        "\n",
        "3. **Performance in Complex Tasks:** Tasks such as object recognition, detection, and segmentation benefit significantly from large receptive fields because these tasks require understanding the spatial relationships and broader context within an image.\n",
        "\n",
        "4. **Network Design:** Understanding the receptive field helps in designing CNN architectures properly, ensuring that neurons in deep layers cover sufficient input to make meaningful decisions without being limited to too small an area.\n",
        "\n",
        "To summarize, the receptive field defines how much of the input each neuron considers, and its growth with depth is critical for enabling CNNs to model complex visual patterns effectively. Without adequately large receptive fields, deep architectures would struggle to capture the necessary context for accurate predictions.\n"
      ],
      "metadata": {
        "id": "Z4PBGY1qE5Ph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The number of parameters in a Convolutional Neural Network (CNN) layer depends primarily on the filter size and the number of filters (also called kernels), but the stride itself does not directly affect the number of parameters.\n",
        "\n",
        "The **filter size** determines how many weights are learned for each filter. Filters are typically small matrices, for example, 3x3 or 5x5, applied to the input or feature map. Each weight in the filter is a parameter to be trained, and there is usually one bias parameter per filter. Thus, the total parameters contributed by one filter are:\n",
        "\n",
        "$$\n",
        "\\text{Parameters per filter} = \\text{filter height} \\times \\text{filter width} \\times \\text{input channels} + 1 \\quad (\\text{bias})\n",
        "$$\n",
        "\n",
        "If the CNN layer has multiple filters, the total trainable parameters in that layer is the above number multiplied by the number of filters.\n",
        "\n",
        "While **filter size** controls how many weights a filter has, **stride** controls how the filter moves along the input during convolution, affecting only the spatial resolution of the output feature map, not the number of parameters learned. A larger stride means the filter jumps over more pixels between applications, producing smaller output maps, but the same filter weights are reused repeatedly, so parameter count remains constant.\n",
        "\n",
        "In summary, **filter size and number of filters directly increase the number of model parameters** since larger filters and more filters mean more weights to learn. Conversely, **stride affects the output feature map size but doesn't change the filter weights or the parameter count**. Choosing bigger filters will increase the model’s capacity but also computation and risk of overfitting, while stride settings influence spatial downsampling and computational efficiency without affecting parameter count.\n",
        "\n",
        "Understanding this distinction is crucial for designing CNNs that balance model complexity and performance.\n"
      ],
      "metadata": {
        "id": "jqBVVdimFqMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# Question 5: Compare and contrast different CNN-based architectures like LeNet,AlexNet, and VGG in terms of depth, filter sizes, and performance.**\n",
        "**Answer:**\n",
        "\n",
        "LeNet, AlexNet, and VGG are pioneering convolutional neural network architectures that played significant roles in the development of deep learning for image recognition, but they differ substantially in their design, depth, filter sizes, and performance.\n",
        "\n",
        "**LeNet** was one of the earliest CNN architectures, designed mainly for handwritten digit recognition (MNIST dataset). It is relatively shallow with about 7 layers, including convolutional, subsampling (pooling), and fully connected layers. LeNet used larger filters, typically $$5 \\times 5$$, and employed average pooling. Its simplicity made it computationally efficient but limited in performance for complex image datasets.\n",
        "\n",
        "**AlexNet** marked a major breakthrough by winning the ImageNet competition in 2012. It is deeper than LeNet with 8 layers, including 5 convolutional layers followed by 3 fully connected layers. AlexNet introduced the use of $$11 \\times 11$$ filters in early layers, then $$5 \\times 5$$ and $$3 \\times 3$$ filters in later layers. It used max pooling instead of average pooling and ReLU activation functions, significantly improving training speed and accuracy. AlexNet also employed dropout to reduce overfitting. It performed excellently on large-scale datasets like ImageNet, achieving much higher accuracy than previous models.[2][1]\n",
        "\n",
        "**VGG** architecture, developed by Visual Geometry Group, further increased depth with networks as deep as 16 or 19 layers (VGG-16, VGG-19). VGG standardized the use of small $$3 \\times 3$$ filters stacked several times, instead of larger filters. This approach allowed the network to capture complex features with fewer parameters than large filters would require. VGG models achieved very high accuracy on ImageNet, outperforming earlier networks like AlexNet, but they require more computational resources and memory due to their depth and number of parameters. VGG also relied heavily on max pooling to reduce spatial dimensions progressively.\n",
        "\n",
        "In summary:\n",
        "\n",
        "- **Depth:** LeNet is shallow, AlexNet deeper, VGG much deeper.\n",
        "- **Filter Sizes:** LeNet uses larger filters (5x5), AlexNet uses a combination with some large filters early (11x11), VGG uses small (3x3) filters consistently.\n",
        "- **Performance:** LeNet is suitable for simple tasks; AlexNet introduced innovations that improved large-scale image classification; VGG further improved accuracy with very deep architectures at the cost of higher computational demand.\n",
        "\n",
        "These architectures represent the evolution of CNN design principles, balancing filter size, depth, and performance to handle increasingly complex vision tasks.\n"
      ],
      "metadata": {
        "id": "Eqp3kJHYGZgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6: Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.**"
      ],
      "metadata": {
        "id": "MYqDwrw1Gxy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess data: reshape and normalize pixel values\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Build CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35wJs0-1HCOP",
        "outputId": "4121f498-e0b0-47c0-9c3e-08bbf9e40035"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 63ms/step - accuracy: 0.8723 - loss: 0.4333 - val_accuracy: 0.9788 - val_loss: 0.0718\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 58ms/step - accuracy: 0.9807 - loss: 0.0614 - val_accuracy: 0.9856 - val_loss: 0.0493\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 61ms/step - accuracy: 0.9878 - loss: 0.0407 - val_accuracy: 0.9858 - val_loss: 0.0476\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 64ms/step - accuracy: 0.9912 - loss: 0.0296 - val_accuracy: 0.9855 - val_loss: 0.0531\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 60ms/step - accuracy: 0.9932 - loss: 0.0218 - val_accuracy: 0.9883 - val_loss: 0.0393\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9887 - loss: 0.0379\n",
            "Test accuracy: 0.9910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create CNN model to classify RGB images. Show your preprocessing and architecture.**"
      ],
      "metadata": {
        "id": "qzJxXasqHH9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Load and preprocess the CIFAR-10 dataset using Keras, and create CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocess data: Normalize pixel values to the range 0-1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define CNN model architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')  # 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69dTOTXfHXtP",
        "outputId": "06c65c2f-4bf1-4abb-d266-c3b92bfdd6e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 177ms/step - accuracy: 0.3492 - loss: 1.7652 - val_accuracy: 0.5511 - val_loss: 1.2500\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 177ms/step - accuracy: 0.5851 - loss: 1.1724 - val_accuracy: 0.6304 - val_loss: 1.0608\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 165ms/step - accuracy: 0.6648 - loss: 0.9553 - val_accuracy: 0.6762 - val_loss: 0.9227\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 166ms/step - accuracy: 0.7078 - loss: 0.8329 - val_accuracy: 0.7016 - val_loss: 0.8634\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 167ms/step - accuracy: 0.7490 - loss: 0.7232 - val_accuracy: 0.7100 - val_loss: 0.8526\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 166ms/step - accuracy: 0.7802 - loss: 0.6376 - val_accuracy: 0.7137 - val_loss: 0.8340\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 174ms/step - accuracy: 0.8016 - loss: 0.5655 - val_accuracy: 0.7205 - val_loss: 0.8339\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 165ms/step - accuracy: 0.8262 - loss: 0.5029 - val_accuracy: 0.7400 - val_loss: 0.7867\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 173ms/step - accuracy: 0.8506 - loss: 0.4298 - val_accuracy: 0.7371 - val_loss: 0.8097\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 166ms/step - accuracy: 0.8679 - loss: 0.3736 - val_accuracy: 0.7300 - val_loss: 0.9025\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.7343 - loss: 0.9229\n",
            "Test accuracy: 0.7232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.**"
      ],
      "metadata": {
        "id": "qR8OLfgNHj5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations for MNIST (Normalize and convert to tensor)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Define CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))   # Conv1 + ReLU\n",
        "        x = self.relu(self.conv2(x))   # Conv2 + ReLU\n",
        "        x = self.pool(x)               # Max Pooling\n",
        "        x = x.view(x.size(0), -1)     # Flatten\n",
        "        x = self.relu(self.fc1(x))    # Fully connected + ReLU\n",
        "        x = self.fc2(x)               # Output layer\n",
        "        return x\n",
        "\n",
        "# Instantiate model, define loss and optimizer\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOm7Mb2NHsD7",
        "outputId": "1f01631c-7716-4cd1-a6dd-6ee1a41939d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.5MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 501kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.03MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.04MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.1220\n",
            "Epoch 2/5, Loss: 0.0372\n",
            "Epoch 3/5, Loss: 0.0231\n",
            "Epoch 4/5, Loss: 0.0153\n",
            "Epoch 5/5, Loss: 0.0102\n",
            "Test accuracy: 0.9856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.**\n"
      ],
      "metadata": {
        "id": "jTFKQ6PzIEoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "train_dir = './dataset/train'  # Path where your training images are located\n",
        "val_dir = './dataset/val'      # Path for validation images\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Define paths for training and validation directories\n",
        "# IMPORTANT: Replace 'path/to/train' and 'path/to/val' with the actual paths to your dataset directories\n",
        "train_dir = 'path/to/train'\n",
        "val_dir = 'path/to/val'\n",
        "\n",
        "# Create ImageDataGenerator instances with augmentation for training and rescaling for validation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load and preprocess images from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Define a simple CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')  # Output layer with number of classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# history = model.fit( # Commented out the training loop as it will fail without valid data paths\n",
        "#     train_generator,\n",
        "#     epochs=10,\n",
        "#     validation_data=validation_generator\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "NNtMdT9eIYVE",
        "outputId": "c7722793-d4de-4f2c-cc11-043df200597a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './dataset/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3213534835.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./dataset/val'\u001b[0m      \u001b[0;31m# Path for validation images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m train_generator = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 10: You are working on a web application for a medical imaging startup. Yourtask is to build and deploy a CNN model that classifies chest X-ray images into “Normal”and “Pneumonia” categories. Describe your end-to-end approach–from data preparation and model training to deploying the model as a web app using Streamlit.**\n",
        "**Answer:**\n",
        "\n",
        "**Step 1: Data Preparation**\n",
        "\n",
        "Collect a labeled dataset of chest X-ray images divided into “Normal” and “Pneumonia” classes.\n",
        "\n",
        "Preprocess images by resizing (e.g., 128x128), normalizing pixel values (0-1), and performing data augmentation (rotation, zoom, flipping) to increase generalization.\n",
        "\n",
        "Split data into training, validation, and testing sets. Use ImageDataGenerator for preprocessing and augmentation.\n",
        "\n",
        "**Step 2: Model Design and Training**\n",
        "\n",
        "Build a CNN architecture suitable for medical image classification, e.g., 2-3 convolutional layers with ReLU activations, max pooling layers, flattening, and fully connected dense layers ending in softmax for binary classification.\n",
        "\n",
        "Compile the model with an appropriate loss function (binary_crossentropy or categorical_crossentropy if one-hot encoded) and optimizer like Adam.\n",
        "\n",
        "Train the model using the training set and validate on the validation set to monitor for overfitting and tune hyperparameters.\n",
        "\n",
        "**Step 3: Model Evaluation and Saving**\n",
        "\n",
        "After training, evaluate the model on the test set to get accuracy, precision, recall, and F1-score.\n",
        "\n",
        "Save the trained model to disk using Keras model.save('xray_classifier.h5').\n",
        "\n",
        "**Step 4: Deployment with Streamlit**\n",
        "\n",
        "Create a simple Streamlit app to upload chest X-ray images and predict using the trained CNN model.\n",
        "\n",
        "Load the saved model in the app and preprocess uploaded images in the same way as training data.\n",
        "\n",
        "Display the prediction label (“Normal” or “Pneumonia”) with the probability score."
      ],
      "metadata": {
        "id": "DjfWZm4UIb66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training and saving snippet (simplified)\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# IMPORTANT: Replace 'data/train' with the actual path to your training data directory\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, zoom_range=0.2, horizontal_flip=True)\n",
        "train_generator = train_datagen.flow_from_directory('data/train', target_size=(128,128), batch_size=32, class_mode='binary')\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# model.fit(train_generator, epochs=10) # Commented out the training loop as it will fail without valid data paths\n",
        "model.save('xray_classifier.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "uZL9BrDRImeL",
        "outputId": "f993e6c1-dc7c-4b10-94f3-cd10c62a059b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1417226595.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# IMPORTANT: Replace 'data/train' with the actual path to your training data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizontal_flip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_datagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m model = models.Sequential([\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlit app snippet for deployment\n",
        "import streamlit as st\n",
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "model = load_model('xray_classifier.h5')\n",
        "st.title(\"Chest X-ray Pneumonia Classifier\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload Chest X-ray\", type=[\"jpg\", \"png\"])\n",
        "if uploaded_file:\n",
        "    img = Image.open(uploaded_file).resize((128,128))\n",
        "    img_array = np.array(img)/255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "    prediction = model.predict(img_array)[0][0]\n",
        "    if prediction > 0.5:\n",
        "        st.write(f\"Prediction: Pneumonia ({prediction:.2f} confidence)\")\n",
        "    else:\n",
        "        st.write(f\"Prediction: Normal ({1-prediction:.2f} confidence)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "w2o9NnGpJN8W",
        "outputId": "8e75c295-681e-4983-ada2-e450d602cf06"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'streamlit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1459539430.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Streamlit app snippet for deployment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}